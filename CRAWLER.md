# Guide du Web Crawler

## Vue d'Ensemble

L'AI Web Reader int√®gre maintenant un **crawler web intelligent** capable de parcourir automatiquement tout un site web au lieu d'une seule page.

## Modes de Scraping

### Mode Single (par d√©faut)
- Scrape **uniquement** la page √† l'URL sp√©cifi√©e
- Rapide et pr√©visible
- Id√©al pour pages uniques ou documentation simple

### Mode Full (nouveau)
- Parcourt **r√©cursivement** toutes les pages du site
- Suit les liens internes automatiquement
- Id√©al pour sites complets, documentation multi-pages, sites d'information

## Configuration

Ajoutez ces lignes dans votre fichier `.env` :

```env
# Active le mode crawler complet
CRAWL_MODE=full

# Nombre maximum de pages √† crawler (s√©curit√©)
MAX_PAGES=100

# Profondeur maximale de liens (0 = page de d√©part uniquement)
MAX_DEPTH=3

# D√©lai entre requ√™tes en secondes (politesse envers le serveur)
CRAWL_DELAY=1.0

# Ne crawler que les liens du m√™me domaine
SAME_DOMAIN_ONLY=true

# Patterns √† exclure (s√©par√©s par virgules)
EXCLUDE_PATTERNS=.pdf,.jpg,.png,.gif,/admin,/login
```

## Exemples de Configuration

### Exemple 1 : Site de Documentation Complet

```env
TARGET_URL=https://docs.example.com
CRAWL_MODE=full
MAX_PAGES=200
MAX_DEPTH=4
CRAWL_DELAY=1.0
SAME_DOMAIN_ONLY=true
EXCLUDE_PATTERNS=.pdf,/api-reference
```

**R√©sultat :** Crawle jusqu'√† 200 pages de documentation, jusqu'√† 4 niveaux de profondeur, en excluant les PDFs.

### Exemple 2 : Site Institutionnel (comme policeliege.be)

```env
TARGET_URL=https://policeliege.be/
CRAWL_MODE=full
MAX_PAGES=50
MAX_DEPTH=2
CRAWL_DELAY=2.0
SAME_DOMAIN_ONLY=true
EXCLUDE_PATTERNS=.pdf,.doc,.docx,/admin,/login
```

**R√©sultat :** Crawle jusqu'√† 50 pages, 2 niveaux de profondeur, avec un d√©lai de 2 secondes (respectueux).

### Exemple 3 : Page Unique (mode classique)

```env
TARGET_URL=https://example.com/specific-page
CRAWL_MODE=single
```

**R√©sultat :** Scrape uniquement la page sp√©cifi√©e (comportement original).

## Param√®tres D√©taill√©s

### CRAWL_MODE
- **Valeurs :** `single` ou `full`
- **D√©faut :** `single`
- **Description :** D√©termine si une seule page ou tout le site est crawl√©

### MAX_PAGES
- **Valeurs :** Entier positif (1-1000)
- **D√©faut :** 100
- **Description :** Nombre maximum de pages √† crawler (limite de s√©curit√©)
- **Recommandation :**
  - Petits sites : 50-100
  - Sites moyens : 100-300
  - Grands sites : 300-1000

### MAX_DEPTH
- **Valeurs :** Entier positif (0-10)
- **D√©faut :** 3
- **Description :** Profondeur maximale de liens √† suivre depuis la page de d√©part
- **Exemples :**
  - `0` : Page de d√©part uniquement
  - `1` : Page de d√©part + liens directs
  - `2` : + liens des pages li√©es
  - `3` : + encore un niveau (recommand√©)

### CRAWL_DELAY
- **Valeurs :** Nombre d√©cimal (0.5-10.0)
- **D√©faut :** 1.0
- **Description :** D√©lai en secondes entre chaque requ√™te
- **Recommandation :**
  - Sites rapides/CDN : 0.5-1.0
  - Sites normaux : 1.0-2.0
  - Sites lents/publics : 2.0-5.0

### SAME_DOMAIN_ONLY
- **Valeurs :** `true` ou `false`
- **D√©faut :** `true`
- **Description :** Ne suivre que les liens du m√™me domaine
- **Recommandation :** Toujours `true` sauf besoins sp√©cifiques

### EXCLUDE_PATTERNS
- **Format :** Liste s√©par√©e par virgules
- **D√©faut :** `.pdf,.jpg,.png,.gif,/admin,/login`
- **Description :** Patterns d'URLs √† exclure du crawling
- **Exemples :**
  - Extensions : `.pdf`, `.doc`, `.zip`
  - Chemins : `/admin`, `/login`, `/private`
  - Sous-domaines : `cdn.`, `media.`

## Fonctionnement du Crawler

1. **D√©marrage** : Le crawler commence √† l'URL de d√©part (TARGET_URL)

2. **Extraction de liens** : Sur chaque page, le crawler extrait tous les liens `<a href="...">`

3. **Filtrage** : Les liens sont filtr√©s selon :
   - Domaine (si SAME_DOMAIN_ONLY=true)
   - Patterns d'exclusion
   - Extensions de fichiers binaires
   - Profondeur maximale

4. **File d'attente** : Les liens valides sont ajout√©s √† une file d'attente (BFS)

5. **Crawling** : Chaque page est visit√©e, avec un d√©lai de politesse entre requ√™tes

6. **Extraction** : Le contenu textuel est extrait et d√©coup√© en chunks

7. **Indexation** : Tous les chunks sont index√©s dans ChromaDB

## Logs et Monitoring

Le crawler produit des logs d√©taill√©s :

```
INFO:WebScraper initialized in 'full' mode for https://example.com
INFO:WebCrawler initialized for https://example.com
INFO:Settings: max_pages=100, max_depth=3, same_domain=True
INFO:Starting crawl from https://example.com
INFO:Crawled [1/100]: https://example.com (depth 0)
INFO:Crawled [2/100]: https://example.com/about (depth 1)
INFO:Crawled [3/100]: https://example.com/services (depth 1)
...
INFO:Full site scrape complete: 45 pages, 312 chunks
INFO:Crawler stats: {'pages_crawled': 45, 'urls_visited': 67, 'max_pages': 100, 'max_depth': 3}
```

## Performance et Bonnes Pratiques

### Optimisation des Performances

1. **Ajustez MAX_PAGES** : Commencez petit (50), augmentez si n√©cessaire
2. **Limitez MAX_DEPTH** : Profondeur 2-3 suffit g√©n√©ralement
3. **Utilisez EXCLUDE_PATTERNS** : √âvitez les sections inutiles
4. **CRAWL_DELAY adapt√©** : Plus court = plus rapide, mais plus agressif

### Bonnes Pratiques

1. **Respectez les serveurs** : Utilisez un d√©lai raisonnable (‚â•1.0s)
2. **Testez d'abord** : Commencez avec MAX_PAGES=10 pour tester
3. **Excluez judicieusement** : M√©dias, admin, auth, API
4. **Surveillez les logs** : V√©rifiez que les bonnes pages sont crawl√©es

### Temps de Crawling Estim√©

Avec CRAWL_DELAY=1.0 :
- 50 pages : ~1 minute
- 100 pages : ~2 minutes
- 200 pages : ~4 minutes
- 500 pages : ~10 minutes

## Exemples d'Usage

### Via le Chat UI

1. √âditez `.env` avec les param√®tres de crawl
2. Lancez `./run_chat.sh`
3. Cliquez sur "üîÑ Mettre √† jour" dans la sidebar
4. Le crawler parcourt le site et affiche la progression

### Via l'API

```bash
# Configurez .env avec CRAWL_MODE=full
curl -X POST http://localhost:8000/update
```

### Via CLI

```bash
# Configurez .env avec CRAWL_MODE=full
python cli.py update
```

## D√©pannage

### Probl√®me : Trop peu de pages crawl√©es

**Causes possibles :**
- MAX_DEPTH trop faible ‚Üí Augmentez √† 3-4
- EXCLUDE_PATTERNS trop restrictifs ‚Üí R√©visez les patterns
- Site avec peu de liens internes ‚Üí Normal

### Probl√®me : Crawler trop lent

**Solutions :**
- R√©duisez CRAWL_DELAY (mais restez poli)
- R√©duisez MAX_PAGES
- R√©duisez MAX_DEPTH

### Probl√®me : Pages ind√©sirables crawl√©es

**Solutions :**
- Ajoutez patterns √† EXCLUDE_PATTERNS
- V√©rifiez SAME_DOMAIN_ONLY=true
- R√©duisez MAX_DEPTH

### Probl√®me : Erreurs SSL

**Solution :**
```env
VERIFY_SSL=false  # Uniquement pour sites de confiance !
```

## Statistiques de Crawling

Apr√®s chaque crawl, les statistiques sont affich√©es :

```
Crawler stats: {
  'pages_crawled': 45,      # Pages effectivement crawl√©es
  'urls_visited': 67,       # URLs totales visit√©es (inclut √©checs)
  'max_pages': 100,         # Limite configur√©e
  'max_depth': 3            # Profondeur max configur√©e
}
```

## Comparaison Mode Single vs Full

| Crit√®re | Mode Single | Mode Full |
|---------|-------------|-----------|
| Pages | 1 seule | Plusieurs (jusqu'√† MAX_PAGES) |
| Temps | Quelques secondes | Quelques minutes |
| Contenu | Limit√© √† une page | Tout le site |
| Complexit√© | Tr√®s simple | Configuration requise |
| Usage | Pages uniques | Documentation, sites complets |
| Chunks typiques | 4-20 | 100-1000+ |

## FAQ

**Q: Puis-je crawler un site externe (pas le mien) ?**
R: Oui, mais respectez le d√©lai (CRAWL_DELAY ‚â• 1.0) et v√©rifiez le robots.txt du site.

**Q: Le crawler respecte-t-il robots.txt ?**
R: Pas automatiquement dans la version actuelle. Ajoutez les chemins interdits dans EXCLUDE_PATTERNS.

**Q: Puis-je crawler plusieurs sites diff√©rents ?**
R: Non, un seul site par configuration. Changez TARGET_URL dans .env pour changer de site.

**Q: Le contenu est-il mis √† jour automatiquement ?**
R: Oui, selon UPDATE_FREQUENCY. Le crawler re-crawle p√©riodiquement.

**Q: Combien de chunks puis-je avoir au maximum ?**
R: Illimit√©, mais ChromaDB fonctionne mieux avec <100,000 chunks. Pour de tr√®s grands sites, augmentez la taille des chunks.

## Support

Pour plus d'aide :
- Consultez les logs d√©taill√©s
- V√©rifiez INSTALLATION.md pour les probl√®mes courants
- Testez avec CRAWL_MODE=single d'abord
- Contactez le support avec les logs complets
