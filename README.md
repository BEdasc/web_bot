# AI Web Reader

Une application IA capable de lire le contenu d'un site web et de r√©pondre √† des questions avec des fonctionnalit√©s anti-hallucination.

## Caract√©ristiques

### Fonctionnalit√©s principales

- **Scraping web intelligent** : Extraction automatique du contenu textuel des sites web
- **Recherche s√©mantique** : Base de donn√©es vectorielle (ChromaDB) pour une recherche efficace
- **Questions-R√©ponses IA** : Utilise Claude (Anthropic) pour r√©pondre aux questions
- **Mise √† jour automatique** : D√©tection des changements et rafra√Æchissement automatique du contenu
- **Anti-hallucination** : Protections robustes contre l'invention de contenu

### Protections anti-hallucination

1. **RAG (Retrieval Augmented Generation)** : Toutes les r√©ponses sont bas√©es uniquement sur le contenu r√©cup√©r√©
2. **Citations obligatoires** : Chaque r√©ponse cite ses sources
3. **V√©rification des sources** : Affichage des extraits exacts utilis√©s
4. **√âvaluation de confiance** : Indicateur de confiance pour chaque r√©ponse
5. **R√©ponse "Je ne sais pas"** : Refuse de r√©pondre si l'information n'existe pas
6. **Pas d'ajout d'informations** : Interdiction d'utiliser des connaissances g√©n√©rales

## Installation

### Pr√©requis

- Python 3.8+
- Cl√© API Anthropic

### √âtapes d'installation

1. Cloner le d√©p√¥t :
```bash
git clone <your-repo-url>
cd web_bot
```

2. Cr√©er un environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate
```

3. Installer les d√©pendances :
```bash
pip install -r requirements.txt
```

4. Configurer l'environnement :
```bash
cp .env.example .env
```

5. √âditer le fichier `.env` avec vos param√®tres :
```env
ANTHROPIC_API_KEY=votre_cl√©_api_ici
TARGET_URL=https://example.com
UPDATE_FREQUENCY=60
```

## Utilisation

### üé® Interface Graphique de Chat (Recommand√© pour les d√©butants)

L'application dispose d'une interface graphique moderne et intuitive !

**D√©marrage rapide:**

```bash
# Linux/Mac
./run_chat.sh

# Windows
run_chat.bat
```

L'interface de chat s'ouvrira automatiquement dans votre navigateur sur `http://localhost:8501`

**Fonctionnalit√©s:**
- üí¨ Chat interactif avec historique
- üìö Affichage des sources avec extraits
- üéØ Indicateurs de confiance color√©s
- ‚öôÔ∏è Configuration en temps r√©el
- üîÑ Mise √† jour manuelle du contenu

**Documentation compl√®te:** Voir [CHAT_UI.md](CHAT_UI.md)

### üîß API REST (Pour les d√©veloppeurs)

**D√©marrage de l'API:**

```bash
python main.py
```

L'API sera disponible sur `http://localhost:8000`

### Documentation API

Une fois l'application d√©marr√©e, acc√©dez √† :
- Documentation interactive : `http://localhost:8000/docs`
- Documentation alternative : `http://localhost:8000/redoc`

### Endpoints API

#### 1. Poser une question

```bash
POST /ask
```

**Exemple de requ√™te** :
```bash
curl -X POST "http://localhost:8000/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Quel est le sujet principal du site?",
    "n_sources": 5
  }'
```

**Exemple de r√©ponse** :
```json
{
  "answer": "Selon Source 1, le site traite de...",
  "sources": [
    {
      "text": "Extrait du texte source...",
      "url": "https://example.com",
      "title": "Titre de la page",
      "relevance_score": 0.85
    }
  ],
  "confidence": "high",
  "question": "Quel est le sujet principal du site?"
}
```

#### 2. Question avec streaming

```bash
POST /ask/stream
```

**Exemple** :
```bash
curl -X POST "http://localhost:8000/ask/stream" \
  -H "Content-Type: application/json" \
  -d '{"question": "Expliquez le contenu principal"}' \
  --no-buffer
```

#### 3. V√©rifier le statut

```bash
GET /status
```

**Exemple de r√©ponse** :
```json
{
  "status": "running",
  "target_url": "https://example.com",
  "last_update": "2024-01-15T10:30:00",
  "update_count": 5,
  "collection_size": 150,
  "update_frequency_minutes": 60
}
```

#### 4. Forcer une mise √† jour

```bash
POST /update
```

```bash
curl -X POST "http://localhost:8000/update"
```

#### 5. Health check

```bash
GET /health
```

## Architecture

```
web_bot/
‚îú‚îÄ‚îÄ config.py           # Gestion de la configuration
‚îú‚îÄ‚îÄ scraper.py          # Module de scraping web
‚îú‚îÄ‚îÄ vector_store.py     # Interface ChromaDB
‚îú‚îÄ‚îÄ qa_engine.py        # Moteur Q&A avec anti-hallucination
‚îú‚îÄ‚îÄ updater.py          # Syst√®me de mise √† jour automatique
‚îú‚îÄ‚îÄ api.py              # API FastAPI
‚îú‚îÄ‚îÄ main.py             # Point d'entr√©e
‚îî‚îÄ‚îÄ requirements.txt    # D√©pendances
```

### Flux de donn√©es

1. **Scraping** : Le `WebScraper` extrait le contenu du site web
2. **Stockage** : Le contenu est divis√© en chunks et stock√© dans ChromaDB
3. **Question** : L'utilisateur pose une question via l'API
4. **Recherche** : Le `VectorStore` trouve les chunks pertinents
5. **G√©n√©ration** : Le `QAEngine` g√©n√®re une r√©ponse bas√©e uniquement sur les sources
6. **R√©ponse** : La r√©ponse avec citations et sources est retourn√©e

### D√©tection des changements

- Hash SHA256 du contenu HTML
- Comparaison automatique √† chaque mise √† jour
- Rafra√Æchissement uniquement si le contenu a chang√©

## Exemples d'utilisation Python

### Exemple de client simple

```python
import requests

API_URL = "http://localhost:8000"

def ask_question(question: str):
    response = requests.post(
        f"{API_URL}/ask",
        json={"question": question, "n_sources": 5}
    )
    result = response.json()

    print(f"Question: {result['question']}")
    print(f"Confiance: {result['confidence']}")
    print(f"\nR√©ponse:\n{result['answer']}\n")

    print("Sources:")
    for i, source in enumerate(result['sources'], 1):
        print(f"\n[Source {i}]")
        print(f"URL: {source['url']}")
        print(f"Extrait: {source['text']}")
        print(f"Score: {source['relevance_score']:.2f}")

# Utilisation
ask_question("Quel est le contenu principal du site?")
```

### Exemple avec streaming

```python
import requests
import json

def ask_with_streaming(question: str):
    response = requests.post(
        f"{API_URL}/ask/stream",
        json={"question": question},
        stream=True
    )

    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if data['type'] == 'answer_chunk':
                print(data['content'], end='', flush=True)
            elif data['type'] == 'sources':
                print("\n\nSources utilis√©es:")
                for source in data['content']:
                    print(f"- {source['title']}: {source['url']}")
                print("\nR√©ponse: ", end='')

ask_with_streaming("Expliquez le contenu")
```

## Configuration avanc√©e

### Variables d'environnement

| Variable | Description | D√©faut |
|----------|-------------|--------|
| `ANTHROPIC_API_KEY` | Cl√© API Anthropic (obligatoire) | - |
| `TARGET_URL` | URL du site web √† scraper (obligatoire) | - |
| `UPDATE_FREQUENCY` | Fr√©quence de mise √† jour en minutes | 60 |
| `CHROMA_PERSIST_DIRECTORY` | R√©pertoire pour ChromaDB | ./chroma_db |
| `API_HOST` | H√¥te de l'API | 0.0.0.0 |
| `API_PORT` | Port de l'API | 8000 |

### Personnalisation du scraping

Pour modifier la taille des chunks ou le comportement du scraper, √©ditez `scraper.py`:

```python
# Modifier la taille des chunks
chunks = self.extract_text_chunks(html, chunk_size=2000)

# Modifier les √©l√©ments HTML √† extraire
for element in soup.find_all(['p', 'h1', 'h2', 'article']):
    # ...
```

## S√©curit√© et bonnes pratiques

1. **Ne commitez jamais votre `.env`** : Contient des cl√©s API sensibles
2. **Limitez les requ√™tes** : Impl√©mentez un rate limiting pour la production
3. **HTTPS en production** : Utilisez toujours HTTPS pour l'API en production
4. **Validez les URLs** : V√©rifiez que les URLs scrapp√©es sont autoris√©es
5. **Monitoring** : Surveillez les logs pour d√©tecter les erreurs

## R√©solution des probl√®mes

### L'application ne d√©marre pas

- V√©rifiez que toutes les d√©pendances sont install√©es
- V√©rifiez que la cl√© API Anthropic est valide
- V√©rifiez les logs pour les erreurs sp√©cifiques

### Pas de r√©ponses ou r√©ponses vides

- V√©rifiez que le site web est accessible
- V√©rifiez qu'au moins une mise √† jour a √©t√© effectu√©e (`/status`)
- Forcez une mise √† jour avec `POST /update`

### Erreurs de scraping

- Certains sites bloquent les scrapers : ajoutez un User-Agent valide
- V√©rifiez les permissions et robots.txt du site
- Certains sites n√©cessitent JavaScript : envisagez d'utiliser Playwright

## Limitations

- Le scraping ne fonctionne que sur les sites statiques ou rendus c√¥t√© serveur
- Les sites n√©cessitant JavaScript peuvent n√©cessiter Playwright
- Le co√ªt de l'API Claude d√©pend du nombre de requ√™tes
- La taille du contenu est limit√©e par ChromaDB et la m√©moire

## Am√©liorations futures

- Support multi-sites
- Cache des r√©ponses fr√©quentes
- Interface web interactive
- Support de fichiers PDF et documents
- Authentification et autorisation
- M√©triques et analytics
- Support multilingue am√©lior√©

## Licence

Voir le fichier LICENSE pour plus de d√©tails.

## Support

Pour des questions ou des probl√®mes, ouvrez une issue sur GitHub.
